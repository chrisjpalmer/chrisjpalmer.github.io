<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Parallelism in Go - Part 2 | Tech Blog</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Hi there! This is a part twoer to my first post on Parallelism in Go. In my last post I explored how goroutine workers can be used to complete IO bound work. The optimum number of goroutines to use is always aligned with the number of units of IO bound work. In this post I set out to explore another time of work which goroutine workers can be used to complete: CPU bound work."><meta name=generator content="Hugo 0.104.0"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/ananke/css/main.min.css><meta property="og:title" content="Parallelism in Go - Part 2"><meta property="og:description" content="Hi there! This is a part twoer to my first post on Parallelism in Go. In my last post I explored how goroutine workers can be used to complete IO bound work. The optimum number of goroutines to use is always aligned with the number of units of IO bound work. In this post I set out to explore another time of work which goroutine workers can be used to complete: CPU bound work."><meta property="og:type" content="article"><meta property="og:url" content="https://chrisjpalmer.github.io/posts/parallelism_in_go_part_2/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-09-04T09:39:01+10:00"><meta property="article:modified_time" content="2022-09-04T09:39:01+10:00"><meta itemprop=name content="Parallelism in Go - Part 2"><meta itemprop=description content="Hi there! This is a part twoer to my first post on Parallelism in Go. In my last post I explored how goroutine workers can be used to complete IO bound work. The optimum number of goroutines to use is always aligned with the number of units of IO bound work. In this post I set out to explore another time of work which goroutine workers can be used to complete: CPU bound work."><meta itemprop=datePublished content="2022-09-04T09:39:01+10:00"><meta itemprop=dateModified content="2022-09-04T09:39:01+10:00"><meta itemprop=wordCount content="2973"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Parallelism in Go - Part 2"><meta name=twitter:description content="Hi there! This is a part twoer to my first post on Parallelism in Go. In my last post I explored how goroutine workers can be used to complete IO bound work. The optimum number of goroutines to use is always aligned with the number of units of IO bound work. In this post I set out to explore another time of work which goroutine workers can be used to complete: CPU bound work."></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">Tech Blog</a><div class="flex-l items-center"><div class=ananke-socials></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">POSTS</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">Parallelism in Go - Part 2</h1><time class="f6 mv4 dib tracked" datetime=2022-09-04T09:39:01+10:00>September 4, 2022</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>Hi there! This is a part twoer to my first post on Parallelism in Go.
In my last post I explored how goroutine workers can be used to complete IO bound work. The optimum number of goroutines to use is always aligned with the number of units of IO bound work. In this post I set out to explore another time of work which goroutine workers can be used to complete: CPU bound work.</p><h2 id=hypothesis>Hypothesis</h2><p>CPU bound work is work which uses the CPU mainly. Examples of this are adding two numbers, iterating in a loop or hashing a value. No network call, system call or disk read is being made and the CPU is being fully utilized. Its also important to note that CPU bound work does not include synchronization with other goroutines (by way of channels or mutexes).</p><p>Unlike IO bound work which waits on something else to complete, CPU bound work consumes CPU cycles. In the IO bound examples we were able to run 3000 goroutines to improve the performance of a workload with 3000 units of IO bound work. The same is not true for CPU bound work. In CPU bound work, we are limited by the number of cores on the machine. If our machine has 4 cores, it can do 4 jobs at once. If our CPU has hyperthreading enabled, then those cores can be kept twice as busy which means we have 8 virtual cores which means it can do 8 jobs at once. These virtual cores are called logical CPUs for simplicity.</p><p>If we think about a normal application running on the operating system, the &ldquo;jobs&rdquo; we are referring to here are threads. Threads are &ldquo;execution contexts&rdquo; that are run in parallel by the CPU. However how does this work in go applications? Go maintains a pool of threads and schedules go routines on top of them. How many threads? It will spin up the same number of goroutines as the number of logical CPUs on your system. This is because spinning up more threads won&rsquo;t achieve any greater parallelism, since the CPU is limited by the number of logical CPUs it has. In fact, adding more threads would actually incur an additional penalty due to context switching. In a go application the threads in the thread pool are referred to as logical processors, and we say that goroutines are &ldquo;scheduled on and off&rdquo; these logical processors.</p><p>When thinking about go applications, goroutines are the concurrency primitive and we don&rsquo;t think about threads. But do the number of goroutines used for parallelising CPU bound work matter? Unlike threads, goroutines do not incur a large penalty when context switched off a logical processor, thanks to their light design. However it should still hold true, that spinning up more goroutines than logical processors, won&rsquo;t achieve more parallelism. We are still limited by the number of logical CPUs available.</p><p>With this in mind, I set out to demonstrate this effect with a few benchmarks. I created some cpu bound work, parallelized using the <code>Do</code> function of the last post, and increased the goroutine workers each time to see how performance was affected. I hypothesized that as you increase workers to the number of logical CPUs, that performance would improve. I also hypothesized that after increasing workers beyond that number, performance would stay the same or eventually get worse.</p><p>This seemed pretty straightforward to me, but after benchmarking I found some pretty weird results&mldr;</p><h2 id=attempt-1>Attempt 1</h2><p>For my first attempt I set up some cpu bound work which hashed an input in a tight loop 10000 times. I chose hashing because it is a CPU intense operation. Since murmur3 is quite performant, I looped it 10000 times to generate some steam.</p><p>Similar to the benchmarking code in the previous post, I first generate some workload, and then call the <code>.Do</code> function increasing the number of workers each time. The <code>Do</code> function is surrounded by another for loop which takes into account <code>b.N</code>. Its important when writing benchmarks to run the target code <code>b.N</code> times so that the go benchmarking runtime can control the number of iterations. It does this to take multiple samples and then average out the results.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>BenchmarkDoCPUBoundWork</span>(<span style=color:#a6e22e>b</span> <span style=color:#f92672>*</span><span style=color:#a6e22e>testing</span>.<span style=color:#a6e22e>B</span>) {
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>workUnits</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>3000</span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>maxWorkers</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>24</span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>bufferSize</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>3000</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#75715e>// create 3000 units of work
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	<span style=color:#a6e22e>work</span> <span style=color:#f92672>:=</span> make([]<span style=color:#66d9ef>string</span>, <span style=color:#a6e22e>workUnits</span>)
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>i</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>0</span>; <span style=color:#a6e22e>i</span> &lt; <span style=color:#a6e22e>workUnits</span>; <span style=color:#a6e22e>i</span><span style=color:#f92672>++</span> {
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>work</span>[<span style=color:#a6e22e>i</span>] = <span style=color:#e6db74>&#34;work&#34;</span> <span style=color:#f92672>+</span> <span style=color:#a6e22e>strconv</span>.<span style=color:#a6e22e>Itoa</span>(<span style=color:#a6e22e>i</span>)
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#75715e>// run a benchmark, increase workers by 1 and run the next benchmark...
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	<span style=color:#75715e>// repeat until we reach 24 workers
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>ws</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>1</span>; <span style=color:#a6e22e>ws</span> <span style=color:#f92672>&lt;=</span> <span style=color:#a6e22e>maxWorkers</span>; <span style=color:#a6e22e>ws</span><span style=color:#f92672>++</span> {
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>b</span>.<span style=color:#a6e22e>Run</span>(<span style=color:#a6e22e>fmt</span>.<span style=color:#a6e22e>Sprintf</span>(<span style=color:#e6db74>&#34;workers %d&#34;</span>, <span style=color:#a6e22e>ws</span>), <span style=color:#66d9ef>func</span>(<span style=color:#a6e22e>b</span> <span style=color:#f92672>*</span><span style=color:#a6e22e>testing</span>.<span style=color:#a6e22e>B</span>) {
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>i</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>0</span>; <span style=color:#a6e22e>i</span> &lt; <span style=color:#a6e22e>b</span>.<span style=color:#a6e22e>N</span>; <span style=color:#a6e22e>i</span><span style=color:#f92672>++</span> {
</span></span><span style=display:flex><span>				<span style=color:#a6e22e>Do</span>(<span style=color:#a6e22e>work</span>, <span style=color:#a6e22e>cpuBoundWorkFunc</span>, <span style=color:#a6e22e>ws</span>, <span style=color:#a6e22e>bufferSize</span>)
</span></span><span style=display:flex><span>			}
</span></span><span style=display:flex><span>		})
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// cpuBoundWorkFunc - does some heavy duty cpu bound work
</span></span></span><span style=display:flex><span><span style=color:#75715e>// hashes the input 10000 times on itself and returns the result
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>cpuBoundWorkFunc</span>(<span style=color:#a6e22e>input</span> <span style=color:#66d9ef>string</span>) (<span style=color:#66d9ef>uint64</span>, <span style=color:#66d9ef>error</span>) {
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>hashLoopCt</span> = <span style=color:#ae81ff>10000</span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>h</span> <span style=color:#f92672>:=</span> <span style=color:#a6e22e>murmur3</span>.<span style=color:#a6e22e>New64</span>()
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>buf</span> <span style=color:#f92672>:=</span> []byte(<span style=color:#a6e22e>input</span>)
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>var</span> <span style=color:#a6e22e>out</span> <span style=color:#66d9ef>uint64</span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>i</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>0</span>; <span style=color:#a6e22e>i</span> &lt; <span style=color:#a6e22e>hashLoopCt</span>; <span style=color:#a6e22e>i</span><span style=color:#f92672>++</span> {
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>h</span>.<span style=color:#a6e22e>Write</span>(<span style=color:#a6e22e>buf</span>)
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>out</span> = <span style=color:#a6e22e>h</span>.<span style=color:#a6e22e>Sum64</span>()
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>buf</span> = []byte(<span style=color:#a6e22e>strconv</span>.<span style=color:#a6e22e>FormatUint</span>(<span style=color:#a6e22e>out</span>, <span style=color:#ae81ff>10</span>))
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> <span style=color:#a6e22e>out</span>, <span style=color:#66d9ef>nil</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>For this attempt I was going to run the benchmarks in 3 environments and compare:</p><ul><li>WSL (Windows Subsystem Linux) - 6 cores</li><li>Windows - 6 cores</li><li>Mac - 8 cores</li></ul><p>My expectation was that performance would increase as you increased workers up to the number of logical CPUs (which on the Mac was 16 and on the Windows and WSL was 12). I was then hoping to see performance decrease a little after that number. The reason why I believed that performance would decrease after workers > logical CPUs is because I am aware that even though goroutines are lightweight, they still incur a penalty for being scheduled on and off logical processors. I was expecting to see the execution time of the benchmark jump up as a result of this penalty.</p><p>I should also note at this point, that I was paying close attention to Dave Chetney&rsquo;s guide on benchmarks. He always suggests to run multiple instances of your benchmarks and then average them out. He also advises that you run some heavy benchmark prior to the real one because CPUs are sometimes lazy and don&rsquo;t perform until you give them a really hefty workload. I did both of these things when gathering my results. Every benchmark run 3 times before moving onto the next number of workers. Additionally I ran the entire benchmark command 3 times. In the end I had 9 results for each worker. Using some spreadsheeting, I took the averages and graphed the results.</p><p><img src=/images/cpubound1.png alt></p><p><img src=/images/cpubound2.png alt></p><p>The results surprised me a little. Yes execution time decreased as you increased goroutine workers. However I was expecting performance to decrease after workers surpassed the number of logical CPUs on each machine. In fact the opposite happened: as workers surpassed logical CPUs performance improved! What&rsquo;s more is that the result for the Mac was showing that the best result was when workers was 8 which was half the number of lofical processors! This was a very weird result and I was puzzled by it.</p><h2 id=experiment-2>Experiment 2</h2><p>I suspected that the test wasn&rsquo;t intense enough so it wasn&rsquo;t pushing the CPU to its maximum capacity. Perhaps when workers were increased beyond logical processors, although theoretically there shouldn&rsquo;t be more throughput due to saturating CPU resources, there was a chance that the CPU was actually turboing at this time, leading to better performance. Modern CPUs are lazy and typically run at a slower clock speed when there isn&rsquo;t much work to do. This is to save power and increase the life of the CPU. This means that unless you are really pushing the CPU to its limits, you may not get the most out of it. With this in mind, I decided to run the experiment again but this time make the CPU do more work!</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-diff data-lang=diff><span style=display:flex><span>func BenchmarkDoCPUBoundWorkV2(b *testing.B) {
</span></span><span style=display:flex><span><span style=color:#f92672>-   workUnits := 3000
</span></span></span><span style=display:flex><span><span style=color:#f92672></span><span style=color:#a6e22e>+   workUnits := 30000
</span></span></span><span style=display:flex><span><span style=color:#a6e22e></span>	...
</span></span><span style=display:flex><span>	// all the same
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Running the tests again these are the results I saw. I only tested these on my Mac and WSL:</p><p><img src=/images/cpubound3.png alt>
<img src=/images/cpubound4.png alt></p><p>Like before, I observed a decrease in execution time as the workers were increased. Once again this was to be expected. The mac result seemed to make more sense now too because unlike the previous results, were execution time increased after 8 workers, this time around it followed the WSL results and trended downwards.</p><p>The problem was however I was still seeing results where workers > logical processors out performed workers = logical processors.
It was at this point I turned to cpu/memory profiles and execution traces to find the answers.</p><p>After comparing several CPU traces I didn&rsquo;t find anything particularly interesting.
However the memory trace was telling me something. There were huge byte array allocations in my code. Over the course of the test the application allocated over 16Gb of memory. Obviously 16Gb wasn&rsquo;t live the whole time (coz it would have been cleaned up by the GC)&mldr; but it led me to think my results could be being impacted by the GC.</p><p>This led me to another aside on the GC where I ended up reading this <a href=https://tip.golang.org/doc/gc-guide>wonderful article</a> on how the GC works. I was searching for some information which might support a new running theory I had: increased workers beyond logical processors, although incurring a scheduler penalty, might actually be benefiting the GC in some way. I still can&rsquo;t be sure whether the GC was the problem with these tests but I did note that the GC mark phase cannot complete until a goroutine is put to sleep. In the case where workers = logical processors, all workers were being kept as busy as possible so they probably didn&rsquo;t want to sleep. In cases like this the GC can issue a penalty to those workers by introducing a write barrier OR also making that worker do something called gcAssist. Gc Assist is when a goroutine is forced by the GC to stop what its doing and participate in the mark phase (mark phase is the phase when the GC discovers all the live allocations). It was possible because all my goroutine workers were so busy that they were actually incurring GC penalties and perhaps a larger number of workers made it easier for the GC to mark memory coz they would sleep more regularly.</p><p>When I viewed the execution traces, I definitely saw that for workers = 24, go routines were frequently switched off logical processors where as for workers = 12, goroutines could stay on for much longer. Could this be a contributing factor&mldr; ? I never dug deep enough to find out.</p><p>However it got me thinking, what if I eliminated the memory factor altogether and made this test as CPU bound as possible.</p><h2 id=experiment-3>Experiment 3</h2><p>My theory was that either allocations or the GC (or both) were contributing to my tests somehow, so I wanted to build a better work function which minimized their effects. I did some analysis on the work function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#75715e>/*1*/</span> <span style=color:#66d9ef>func</span> <span style=color:#a6e22e>cpuBoundWorkFunc</span>(<span style=color:#a6e22e>input</span> <span style=color:#66d9ef>string</span>) (<span style=color:#66d9ef>uint64</span>, <span style=color:#66d9ef>error</span>) {
</span></span><span style=display:flex><span><span style=color:#75715e>/*2*/</span> 	<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>hashLoopCt</span> = <span style=color:#ae81ff>10000</span>
</span></span><span style=display:flex><span><span style=color:#75715e>/*3*/</span> 	<span style=color:#a6e22e>h</span> <span style=color:#f92672>:=</span> <span style=color:#a6e22e>murmur3</span>.<span style=color:#a6e22e>New64</span>()
</span></span><span style=display:flex><span><span style=color:#75715e>/*4*/</span> 	<span style=color:#a6e22e>buf</span> <span style=color:#f92672>:=</span> []byte(<span style=color:#a6e22e>input</span>)
</span></span><span style=display:flex><span><span style=color:#75715e>/*5*/</span> 	<span style=color:#66d9ef>var</span> <span style=color:#a6e22e>out</span> <span style=color:#66d9ef>uint64</span>
</span></span><span style=display:flex><span><span style=color:#75715e>/*6*/</span> 	<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>i</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>0</span>; <span style=color:#a6e22e>i</span> &lt; <span style=color:#a6e22e>hashLoopCt</span>; <span style=color:#a6e22e>i</span><span style=color:#f92672>++</span> {
</span></span><span style=display:flex><span><span style=color:#75715e>/*7*/</span> 		<span style=color:#a6e22e>h</span>.<span style=color:#a6e22e>Write</span>(<span style=color:#a6e22e>buf</span>)
</span></span><span style=display:flex><span><span style=color:#75715e>/*8*/</span> 		<span style=color:#a6e22e>out</span> = <span style=color:#a6e22e>h</span>.<span style=color:#a6e22e>Sum64</span>()
</span></span><span style=display:flex><span><span style=color:#75715e>/*9*/</span> 		<span style=color:#a6e22e>buf</span> = []byte(<span style=color:#a6e22e>strconv</span>.<span style=color:#a6e22e>FormatUint</span>(<span style=color:#a6e22e>out</span>, <span style=color:#ae81ff>10</span>))
</span></span><span style=display:flex><span><span style=color:#75715e>/*10*/</span> 	}
</span></span><span style=display:flex><span><span style=color:#75715e>/*11*/</span> 	<span style=color:#66d9ef>return</span> <span style=color:#a6e22e>out</span>, <span style=color:#66d9ef>nil</span>
</span></span><span style=display:flex><span><span style=color:#75715e>/*12*/</span> }
</span></span></code></pre></div><p>I found:</p><ol><li>on line 9, I was allocating a string on every iteration of the loop</li><li>on line 4, I was allocating a byte array every time <code>cpuBoundWorkFunc</code> was called</li></ol><p>Whats more is after doing some escape analysis, I also found that the buffer being passed to <code>h.Write(buf)</code> was escaping to the heap:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>go build -gcflags<span style=color:#f92672>=</span>-m<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span> github.com/spaolacci/murmur3
</span></span><span style=display:flex><span>go test -gcflags<span style=color:#f92672>=</span>-m<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span> -c ./playground/parallelism_in_go/parallel_cpu_bound_2_test.go
</span></span></code></pre></div><p>This was apparently happening due to the internals of murmur3. Specifically somewhere internally a range was being taken from this byte slice. This is grounds for Go to allocate the byte array to the heap so it was escaping to the heap.</p><p>I wanted to eliminate heap allocations so I dreamed a new <code>Do</code> function that could maintain a shared state per worker goroutine:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#75715e>//parallel.go
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>DoWithState</span>[<span style=color:#a6e22e>I</span> <span style=color:#a6e22e>any</span>, <span style=color:#a6e22e>O</span> <span style=color:#a6e22e>any</span>, <span style=color:#a6e22e>S</span> <span style=color:#a6e22e>any</span>](<span style=color:#a6e22e>work</span> []<span style=color:#a6e22e>I</span>, <span style=color:#a6e22e>stateFunc</span> <span style=color:#a6e22e>StateFunc</span>[<span style=color:#a6e22e>S</span>], <span style=color:#a6e22e>workFunc</span> <span style=color:#a6e22e>WorkFuncWithState</span>[<span style=color:#a6e22e>S</span>, <span style=color:#a6e22e>I</span>, <span style=color:#a6e22e>O</span>], <span style=color:#a6e22e>workers</span> <span style=color:#66d9ef>int</span>, <span style=color:#a6e22e>bufferSize</span> <span style=color:#66d9ef>int</span>) []<span style=color:#a6e22e>Result</span>[<span style=color:#a6e22e>I</span>, <span style=color:#a6e22e>O</span>] {
</span></span><span style=display:flex><span>	<span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>var</span> <span style=color:#a6e22e>wg</span> <span style=color:#a6e22e>sync</span>.<span style=color:#a6e22e>WaitGroup</span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>wg</span>.<span style=color:#a6e22e>Add</span>(<span style=color:#a6e22e>workers</span>)
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>i</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>0</span>; <span style=color:#a6e22e>i</span> &lt; <span style=color:#a6e22e>workers</span>; <span style=color:#a6e22e>i</span><span style=color:#f92672>++</span> {
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>go</span> <span style=color:#66d9ef>func</span>() {
</span></span><span style=display:flex><span>			<span style=color:#a6e22e>s</span> <span style=color:#f92672>:=</span> <span style=color:#a6e22e>stateFunc</span>() <span style=color:#75715e>// create a shared state when the worker goroutine starts.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>			<span style=color:#66d9ef>defer</span> <span style=color:#a6e22e>wg</span>.<span style=color:#a6e22e>Done</span>()
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>w</span> <span style=color:#f92672>:=</span> <span style=color:#66d9ef>range</span> <span style=color:#a6e22e>workC</span> {
</span></span><span style=display:flex><span>				<span style=color:#a6e22e>o</span>, <span style=color:#a6e22e>err</span> <span style=color:#f92672>:=</span> <span style=color:#a6e22e>workFunc</span>(<span style=color:#a6e22e>s</span>, <span style=color:#a6e22e>w</span>) <span style=color:#75715e>// pass that shared state to the work function
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>				<span style=color:#a6e22e>resultC</span> <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>Result</span>[<span style=color:#a6e22e>I</span>, <span style=color:#a6e22e>O</span>]{<span style=color:#a6e22e>w</span>, <span style=color:#a6e22e>o</span>, <span style=color:#a6e22e>err</span>}
</span></span><span style=display:flex><span>			}
</span></span><span style=display:flex><span>		}()
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Then I created a new work function and a function to initialize the shared state:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>cpuBoundWorkFuncV3State</span>() []<span style=color:#66d9ef>byte</span> {
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> make([]<span style=color:#66d9ef>byte</span>, <span style=color:#ae81ff>8</span>)
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>cpuBoundWorkFuncV3</span>(<span style=color:#a6e22e>byteArray</span> []<span style=color:#66d9ef>byte</span>, <span style=color:#a6e22e>input</span> <span style=color:#66d9ef>uint64</span>) (<span style=color:#66d9ef>uint64</span>, <span style=color:#66d9ef>error</span>) {
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>hashLoopCt</span> = <span style=color:#ae81ff>10000</span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>i</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>0</span>; <span style=color:#a6e22e>i</span> &lt; <span style=color:#a6e22e>hashLoopCt</span>; <span style=color:#a6e22e>i</span><span style=color:#f92672>++</span> {
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>binary</span>.<span style=color:#a6e22e>LittleEndian</span>.<span style=color:#a6e22e>PutUint64</span>(<span style=color:#a6e22e>byteArray</span>, <span style=color:#a6e22e>input</span>)
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>input</span> = <span style=color:#a6e22e>murmur3</span>.<span style=color:#a6e22e>Sum64</span>(<span style=color:#a6e22e>byteArray</span>)
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> <span style=color:#a6e22e>input</span>, <span style=color:#66d9ef>nil</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>My new work function would obviously not produce the same output, but it was more or less doing the same thing and this time avoiding allocations on every iteration of the loop. I hoped this would eliminate the noise factor which I hypothesized was coming from the GC.</p><p>Here were my results:</p><p><img src=/images/cpubound5.png alt></p><p><img src=/images/cpubound6.png alt></p><p>I noticed right away this strange peak at workers = 2 than hadn&rsquo;t cropped up before. I found this weird since it didn&rsquo;t make sense that an increase in parallelism was also increasing execution time! Oh well, it certainly revealed that this work function had very different characteristics to the last one!</p><p>Without fail however, I continued to see execution time trend downwards after workers > logical processors! It couldn&rsquo;t be alluded!</p><p>Once again I turned to cpu profiles, memory profiles and execution traces.</p><p>This time the memory trace was very boring. Hardly any memory was being allocated for the whole test. However when comparing CPU profiles between workers = 12 and workers = 24, I couldn&rsquo;t see any obvious reason why workers = 24 was faster than the other. I was so baffled by this result I started to wonder whether somehow ramping the workers up to 24 was actually &ldquo;warming&rdquo; up the CPU better, making the workers = 24 test run faster. To be sure, I actually ran the tests backwards:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>BenchmarkDoCPUBoundWorkV3Backwards</span>(<span style=color:#a6e22e>b</span> <span style=color:#f92672>*</span><span style=color:#a6e22e>testing</span>.<span style=color:#a6e22e>B</span>) {
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>workUnits</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>300</span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>maxWorkers</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>24</span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>bufferSize</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>3000</span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>work</span> <span style=color:#f92672>:=</span> make([]<span style=color:#66d9ef>uint64</span>, <span style=color:#a6e22e>workUnits</span>)
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>i</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>0</span>; <span style=color:#a6e22e>i</span> &lt; <span style=color:#a6e22e>workUnits</span>; <span style=color:#a6e22e>i</span><span style=color:#f92672>++</span> {
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>work</span>[<span style=color:#a6e22e>i</span>] = uint64(<span style=color:#a6e22e>i</span>)
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>ws</span> <span style=color:#f92672>:=</span> <span style=color:#a6e22e>maxWorkers</span>; <span style=color:#a6e22e>ws</span> &gt; <span style=color:#ae81ff>0</span>; <span style=color:#a6e22e>ws</span><span style=color:#f92672>--</span> {
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>b</span>.<span style=color:#a6e22e>Run</span>(<span style=color:#a6e22e>fmt</span>.<span style=color:#a6e22e>Sprintf</span>(<span style=color:#e6db74>&#34;workers %d&#34;</span>, <span style=color:#a6e22e>ws</span>), <span style=color:#66d9ef>func</span>(<span style=color:#a6e22e>b</span> <span style=color:#f92672>*</span><span style=color:#a6e22e>testing</span>.<span style=color:#a6e22e>B</span>) {
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>i</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>0</span>; <span style=color:#a6e22e>i</span> &lt; <span style=color:#a6e22e>b</span>.<span style=color:#a6e22e>N</span>; <span style=color:#a6e22e>i</span><span style=color:#f92672>++</span> {
</span></span><span style=display:flex><span>				<span style=color:#a6e22e>DoWithState</span>(<span style=color:#a6e22e>work</span>, <span style=color:#a6e22e>cpuBoundWorkFuncV3State</span>, <span style=color:#a6e22e>cpuBoundWorkFuncV3</span>, <span style=color:#a6e22e>ws</span>, <span style=color:#a6e22e>bufferSize</span>)
</span></span><span style=display:flex><span>			}
</span></span><span style=display:flex><span>		})
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Astonishingly I got the same results. For some unknown reason, workers = 24 just performed better.</p><p>I started to look into the CPU profiles of individual lines of the murmur hash function. This is where I got another hunch. It seemed that between 12 and 24 worker tests particular lines of code in murmur3 just performed better for workers = 24. I wondered if this might have something to do with alignment of memory and cache lines. Its impossible to know because I just don&rsquo;t have that much in-depth knowledge. However it gave me one lust hunch&mldr; perhaps murmur3 is just such a complex function itself, that its creating situations where for no apparent reason a higher number of workers favour it. This led me to one last test&mldr;</p><h3 id=experiment-4>Experiment 4</h3><p>No more murmur.. this time I made the simplest work function conceivable with 0 memory allocations:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>cpuBoundWorkFuncV4</span>(<span style=color:#a6e22e>input</span> <span style=color:#66d9ef>uint64</span>) (<span style=color:#66d9ef>uint64</span>, <span style=color:#66d9ef>error</span>) {
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>var</span> <span style=color:#a6e22e>i</span> <span style=color:#66d9ef>uint64</span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>var</span> <span style=color:#a6e22e>x</span> <span style=color:#66d9ef>uint64</span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>for</span> ; <span style=color:#a6e22e>i</span> &lt; <span style=color:#ae81ff>1000000</span>; <span style=color:#a6e22e>i</span><span style=color:#f92672>++</span> {
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>x</span> = <span style=color:#a6e22e>i</span> <span style=color:#f92672>%</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> <span style=color:#a6e22e>x</span>, <span style=color:#66d9ef>nil</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>This function does nothing more than operate in a tight loop. The only reason I added lines like <code>x = i % 2</code> was to prevent the go compiler from optimising out my variables. I thought go probably doesn&rsquo;t know how to optimize the result of <code>x</code> in this situation so this would be a good way to make the CPU do some work.</p><p>I ran the test and these were my results:</p><p><img src=/images/cpubound8.png alt>
<img src=/images/cpubound9.png alt>
<img src=/images/cpubound10.png alt></p><p>The test results showed something different this time. Instead of the execution time trending down as workers increased beyond 12, it was trending up. Still the optimium number of worker go routines was not 12, however at least I was starting to see that additional go routines was incurring a schedule penalty as predicted.</p><p>In fact the results were even more apparent when I ran it all the way up to 100 workers:
<img src=/images/cpubound11.png alt></p><p>This result got me pretty excited. So I ran with even more workers and performed 3 test runs to make sure I wasn&rsquo;t running into any noise from the machine:</p><p><img src=/images/cpubound12.png alt></p><p>.. and more workers:</p><p><img src=/images/cpubound13.png alt></p><p>Okay finally I could see what I was looking for! Taking the trendline each time, I found that there was a 0.0005ms (500μs) scheduler penalty per goroutine.
As for the magic number 12, I could not find it. In repeats of the test it just didn&rsquo;t show up:</p><p><img src=/images/cpubound14.png alt>
<img src=/images/cpubound15.png alt>
<img src=/images/cpubound16.png alt></p><p>However what I did notice is that there seems to be random noise in all the results. For example, comparing 3 individual back to back runs of the test, the results don&rsquo;t exactly align with each other:
<img src=/images/cpubound17.png alt></p><p>This suggested to me that perhaps its impossible to see the 500μs penalty between workers = 12 and workers = 13. Perhaps that just wasn&rsquo;t going to possible given that my personal computer always has some degree of noise.</p><h2 id=conclusion>Conclusion</h2><p>Although I coulodn&rsquo;t find the magic number workers = logical processors, I can see with a reasonable degree of confidence that workers = logical processors is the point at which you get the most reasonable optimization with CPU bound work.s</p><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://chrisjpalmer.github.io>&copy; Tech Blog 2022</a><div><div class=ananke-socials></div></div></div></footer></body></html>