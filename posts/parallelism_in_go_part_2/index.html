<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><title>Parallelism in Go - Part 2 | Tech Blog</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Hi there! This is a part twoer to my first post on Parallelism in Go. In my first post I explored parallelism with IO Bound Work - a particular type of workload which Go is very well designed to handle.
In this post I wanted to explore go&rsquo;s parallelism characteristics around CPU Bound work. Now disclaimer: I went into this experiment fishing for a particular result, and I kind of only started to see it after a lot of testing&mldr; I mean a lot."><meta name=generator content="Hugo 0.104.0"><meta name=robots content="noindex, nofollow"><link rel=stylesheet href=/ananke/css/main.min.css><meta property="og:title" content="Parallelism in Go - Part 2"><meta property="og:description" content="Hi there! This is a part twoer to my first post on Parallelism in Go. In my first post I explored parallelism with IO Bound Work - a particular type of workload which Go is very well designed to handle.
In this post I wanted to explore go&rsquo;s parallelism characteristics around CPU Bound work. Now disclaimer: I went into this experiment fishing for a particular result, and I kind of only started to see it after a lot of testing&mldr; I mean a lot."><meta property="og:type" content="article"><meta property="og:url" content="https://chrisjpalmer.github.io/posts/parallelism_in_go_part_2/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-09-04T09:39:01+10:00"><meta property="article:modified_time" content="2022-09-04T09:39:01+10:00"><meta itemprop=name content="Parallelism in Go - Part 2"><meta itemprop=description content="Hi there! This is a part twoer to my first post on Parallelism in Go. In my first post I explored parallelism with IO Bound Work - a particular type of workload which Go is very well designed to handle.
In this post I wanted to explore go&rsquo;s parallelism characteristics around CPU Bound work. Now disclaimer: I went into this experiment fishing for a particular result, and I kind of only started to see it after a lot of testing&mldr; I mean a lot."><meta itemprop=datePublished content="2022-09-04T09:39:01+10:00"><meta itemprop=dateModified content="2022-09-04T09:39:01+10:00"><meta itemprop=wordCount content="2736"><meta itemprop=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Parallelism in Go - Part 2"><meta name=twitter:description content="Hi there! This is a part twoer to my first post on Parallelism in Go. In my first post I explored parallelism with IO Bound Work - a particular type of workload which Go is very well designed to handle.
In this post I wanted to explore go&rsquo;s parallelism characteristics around CPU Bound work. Now disclaimer: I went into this experiment fishing for a particular result, and I kind of only started to see it after a lot of testing&mldr; I mean a lot."></head><body class="ma0 avenir bg-near-white"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l justify-between items-center center"><a href=/ class="f3 fw2 hover-white no-underline white-90 dib">Tech Blog</a><div class="flex-l items-center"><div class=ananke-socials></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l flex-wrap justify-between mw8 center ph3"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked">POSTS</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">Parallelism in Go - Part 2</h1><time class="f6 mv4 dib tracked" datetime=2022-09-04T09:39:01+10:00>September 4, 2022</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>Hi there! This is a part twoer to my first post on Parallelism in Go.
In my first post I explored parallelism with IO Bound Work - a particular type of workload which Go is very well designed to handle.</p><p>In this post I wanted to explore go&rsquo;s parallelism characteristics around CPU Bound work. Now disclaimer: I went into this experiment fishing for a particular result, and I kind of only started to see it after a lot of testing&mldr; I mean a lot. And I just want to be honest about this because this is how it is in the software world - a lot of trial and error. Yes the answers exist out there, but there is often a journey which must be undertaken to discover them first.</p><p>So let me take you through that journey&mldr;</p><h2 id=hypothesis>Hypothesis</h2><p>I have been doing Go for a while and read numerous articles about parallelism over the years. Some of my favourites are the Arden Labs ones. Please check them out :)</p><p>The general consensus behind CPU bound work is this: You can&rsquo;t get more throughput than the number of logical processors.</p><p>Lets break that down. What is a logical processor?</p><p>A logical processor is essentially an execution context where your go programming can get some work done, and in real world terms this translates to a thread. The reason Go chooses to call them &ldquo;logical processors&rdquo; is because Go only spins up a small pool of them. Specifically it spins up as many as there are cores on your computer. And if your computer has hyperthreading enabled, it spins up twice as many logical processors. This means on my 6 core Windows Machine, Go always spins up 12 logical processors. The thought bubble behind this is that, spinning up any more than this will yield no more improvement in throughput due to parallelism. Go is recognising the physical limitations of the machine its running on, so its not going to bother pushing beyond that. In fact, pushing beyond that would theoretically degrade performance because you would incur additional overheads from context switching threads as the CPU tries to give all threads equal time.</p><p>If the physical resources are the limitations for logical processors, then it makes sense that the logical processors are the limiting factor for how many goroutines you can spin up to perform CPU bound work. Now the key here is &ldquo;Cpu Bound&rdquo; because IO Bound work doesn&rsquo;t use the CPU. The goroutine waiting for the IO bound operation (network, disk, syscall) is put to sleep and another goroutine is put in its place on that logical processor. Conversely with CPU bound work, we are referring to work loads which primarily use the CPU. Examples of such workloads are iterating a loop, adding two numbers together or hashing a string. In the case of CPU bound work, given that the CPU is the limiting factor it makes sense that you shouldn&rsquo;t be able to run more goroutines than logical processors, and experience an increase in throughput.</p><p>I thought this would be very easy to prove, so I quickly drafted up my next bench mark experiment to show it in action. What I found is that, this wasn&rsquo;t so easy to prove after all and the story is a little more complicated&mldr; thats okay I like complicated :)</p><h2 id=experiment-1>Experiment 1</h2><p>Using the same parallel <code>Do</code> function in the previous blog post (which you can find in the <a href>examples</a>), I prepared my next victim:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>BenchmarkDoCPUBoundWork</span>(<span style=color:#a6e22e>b</span> <span style=color:#f92672>*</span><span style=color:#a6e22e>testing</span>.<span style=color:#a6e22e>B</span>) {
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>workUnits</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>3000</span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>maxWorkers</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>24</span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>bufferSize</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>3000</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#75715e>// create 3000 units of work
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	<span style=color:#a6e22e>work</span> <span style=color:#f92672>:=</span> make([]<span style=color:#66d9ef>string</span>, <span style=color:#a6e22e>workUnits</span>)
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>i</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>0</span>; <span style=color:#a6e22e>i</span> &lt; <span style=color:#a6e22e>workUnits</span>; <span style=color:#a6e22e>i</span><span style=color:#f92672>++</span> {
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>work</span>[<span style=color:#a6e22e>i</span>] = <span style=color:#e6db74>&#34;work&#34;</span> <span style=color:#f92672>+</span> <span style=color:#a6e22e>strconv</span>.<span style=color:#a6e22e>Itoa</span>(<span style=color:#a6e22e>i</span>)
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=color:#75715e>// run a benchmark, increase workers by 1 and run the next benchmark...
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	<span style=color:#75715e>// repeat until we reach 24 workers
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>	<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>ws</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>1</span>; <span style=color:#a6e22e>ws</span> <span style=color:#f92672>&lt;=</span> <span style=color:#a6e22e>maxWorkers</span>; <span style=color:#a6e22e>ws</span><span style=color:#f92672>++</span> {
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>b</span>.<span style=color:#a6e22e>Run</span>(<span style=color:#a6e22e>fmt</span>.<span style=color:#a6e22e>Sprintf</span>(<span style=color:#e6db74>&#34;workers %d&#34;</span>, <span style=color:#a6e22e>ws</span>), <span style=color:#66d9ef>func</span>(<span style=color:#a6e22e>b</span> <span style=color:#f92672>*</span><span style=color:#a6e22e>testing</span>.<span style=color:#a6e22e>B</span>) {
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>i</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>0</span>; <span style=color:#a6e22e>i</span> &lt; <span style=color:#a6e22e>b</span>.<span style=color:#a6e22e>N</span>; <span style=color:#a6e22e>i</span><span style=color:#f92672>++</span> {
</span></span><span style=display:flex><span>				<span style=color:#a6e22e>Do</span>(<span style=color:#a6e22e>work</span>, <span style=color:#a6e22e>cpuBoundWorkFunc</span>, <span style=color:#a6e22e>ws</span>, <span style=color:#a6e22e>bufferSize</span>)
</span></span><span style=display:flex><span>			}
</span></span><span style=display:flex><span>		})
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>// cpuBoundWorkFunc - does some heavy duty cpu bound work
</span></span></span><span style=display:flex><span><span style=color:#75715e>// hashes the input 10000 times on itself and returns the result
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>cpuBoundWorkFunc</span>(<span style=color:#a6e22e>input</span> <span style=color:#66d9ef>string</span>) (<span style=color:#66d9ef>uint64</span>, <span style=color:#66d9ef>error</span>) {
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>hashLoopCt</span> = <span style=color:#ae81ff>10000</span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>h</span> <span style=color:#f92672>:=</span> <span style=color:#a6e22e>murmur3</span>.<span style=color:#a6e22e>New64</span>()
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>buf</span> <span style=color:#f92672>:=</span> []byte(<span style=color:#a6e22e>input</span>)
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>var</span> <span style=color:#a6e22e>out</span> <span style=color:#66d9ef>uint64</span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>i</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>0</span>; <span style=color:#a6e22e>i</span> &lt; <span style=color:#a6e22e>hashLoopCt</span>; <span style=color:#a6e22e>i</span><span style=color:#f92672>++</span> {
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>h</span>.<span style=color:#a6e22e>Write</span>(<span style=color:#a6e22e>buf</span>)
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>out</span> = <span style=color:#a6e22e>h</span>.<span style=color:#a6e22e>Sum64</span>()
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>buf</span> = []byte(<span style=color:#a6e22e>strconv</span>.<span style=color:#a6e22e>FormatUint</span>(<span style=color:#a6e22e>out</span>, <span style=color:#ae81ff>10</span>))
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> <span style=color:#a6e22e>out</span>, <span style=color:#66d9ef>nil</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>I ran the benchmarks on all three environments I tested in the last post:</p><ul><li>WSL running on my windows - 6 cores</li><li>Windows - 6 cores</li><li>Mac - 8 cores</li></ul><p>Here were the results I got:</p><p><img src=../images/cpubound1.png alt></p><p>The first observation I made from these results were that, predictably, as you increase the workers, the overall time decreased.
This makes sense since you are utilizing the system resources in parallel as you increase workers, therefore gettinga time advantage.</p><p>What I was hoping to see as well was a clear and obvious plateau in performance once you increased workers to the same number of logical processors that each machine had.
For Mac this was 16 and for Windows and WSL this was 12:</p><p><img src=../images/cpubound2.png alt></p><p>This is where the results were not so kind to me. There was really no relationship between the count of logical processors and workers.
For Windows, yes it performed well at 12 (better than previous results), but saw even better results as you increased the workers beyond 12!
For WSL, it performed worse at 12 than it did for 7 and 24.
Similarly for Mac, it performed best at 8 and worse after 13!</p><p>I could not understand why I got these results&mldr; why isn&rsquo;t the magic number = logical processors!</p><p>This led me to another experiment</p><h2 id=experiment-2>Experiment 2</h2><p>I suspected that the test wasn&rsquo;t intense enough so it wasn&rsquo;t pushing the CPU to its maximum capacity. Perhaps when workers were increased beyond logical processors, although theoretically there shouldn&rsquo;t be more throughput due to saturating CPU resources, there was a chance that the CPU was actually turboing at this time, leading to better performance. Modern CPUs are lazy and typically run at a slower clock speed when there isn&rsquo;t much work to do. This is to save power and increase the life of the CPU. This means that unless you are really pushing the CPU to its limits, you may not get the most out of it. With this in mind, I decided to run the experiment again but this time make the CPU do more work!</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-diff data-lang=diff><span style=display:flex><span>func BenchmarkDoCPUBoundWorkV2(b *testing.B) {
</span></span><span style=display:flex><span><span style=color:#f92672>-   workUnits := 3000
</span></span></span><span style=display:flex><span><span style=color:#f92672></span><span style=color:#a6e22e>+   workUnits := 30000
</span></span></span><span style=display:flex><span><span style=color:#a6e22e></span>	...
</span></span><span style=display:flex><span>	// all the same
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Running the tests again these are the results I saw. I only tested these on my Mac and WSL:</p><p><img src=../images/cpubound3.png alt>
<img src=../images/cpubound4.png alt></p><p>Like before, I observed a decrease in execution time as the workers were increased. Once again this was to be expected. The mac result seemed to make more sense now too because unlike the previous results, were execution time increased after 8 workers, this time around it followed the WSL results and trended downwards.</p><p>The problem was however I was still seeing results where workers > logical processors out performed workers = logical processors.
It was at this point I turned to cpu/memory profiles and execution traces to find the answers.</p><p>After comparing several CPU traces I didn&rsquo;t find anything particularly interesting.
However the memory trace was telling me something. There were huge byte array allocations in my code. Over the course of the test the application allocated over 16Gb of memory. Obviously 16Gb wasn&rsquo;t live the whole time (coz it would have been cleaned up by the GC)&mldr; but it led me to think my results could be being impacted by the GC.</p><p>This led me to another aside on the GC where I ended up reading this <a href=https://tip.golang.org/doc/gc-guide>wonderful article</a> on how the GC works. I was searching for some information which might support a new running theory I had: increased workers beyond logical processors, although incurring a scheduler penalty, might actually be benefiting the GC in some way. I still can&rsquo;t be sure whether the GC was the problem with these tests but I did note that the GC mark phase cannot complete until a goroutine is put to sleep. In the case where workers = logical processors, all workers were being kept as busy as possible so they probably didn&rsquo;t want to sleep. In cases like this the GC can issue a penalty to those workers by introducing a write barrier OR also making that worker do something called gcAssist. Gc Assist is when a goroutine is forced by the GC to stop what its doing and participate in the mark phase (mark phase is the phase when the GC discovers all the live allocations). It was possible because all my goroutine workers were so busy that they were actually incurring GC penalties and perhaps a larger number of workers made it easier for the GC to mark memory coz they would sleep more regularly.</p><p>When I viewed the execution traces, I definitely saw that for workers = 24, go routines were frequently switched off logical processors where as for workers = 12, goroutines could stay on for much longer. Could this be a contributing factor&mldr; ? I never dug deep enough to find out.</p><p>However it got me thinking, what if I eliminated the memory factor altogether and made this test as CPU bound as possible.</p><h2 id=experiment-3>Experiment 3</h2><p>My theory was that either allocations or the GC (or both) were contributing to my tests somehow, so I wanted to build a better work function which minimized their effects. I did some analysis on the work function:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#75715e>/*1*/</span> <span style=color:#66d9ef>func</span> <span style=color:#a6e22e>cpuBoundWorkFunc</span>(<span style=color:#a6e22e>input</span> <span style=color:#66d9ef>string</span>) (<span style=color:#66d9ef>uint64</span>, <span style=color:#66d9ef>error</span>) {
</span></span><span style=display:flex><span><span style=color:#75715e>/*2*/</span> 	<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>hashLoopCt</span> = <span style=color:#ae81ff>10000</span>
</span></span><span style=display:flex><span><span style=color:#75715e>/*3*/</span> 	<span style=color:#a6e22e>h</span> <span style=color:#f92672>:=</span> <span style=color:#a6e22e>murmur3</span>.<span style=color:#a6e22e>New64</span>()
</span></span><span style=display:flex><span><span style=color:#75715e>/*4*/</span> 	<span style=color:#a6e22e>buf</span> <span style=color:#f92672>:=</span> []byte(<span style=color:#a6e22e>input</span>)
</span></span><span style=display:flex><span><span style=color:#75715e>/*5*/</span> 	<span style=color:#66d9ef>var</span> <span style=color:#a6e22e>out</span> <span style=color:#66d9ef>uint64</span>
</span></span><span style=display:flex><span><span style=color:#75715e>/*6*/</span> 	<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>i</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>0</span>; <span style=color:#a6e22e>i</span> &lt; <span style=color:#a6e22e>hashLoopCt</span>; <span style=color:#a6e22e>i</span><span style=color:#f92672>++</span> {
</span></span><span style=display:flex><span><span style=color:#75715e>/*7*/</span> 		<span style=color:#a6e22e>h</span>.<span style=color:#a6e22e>Write</span>(<span style=color:#a6e22e>buf</span>)
</span></span><span style=display:flex><span><span style=color:#75715e>/*8*/</span> 		<span style=color:#a6e22e>out</span> = <span style=color:#a6e22e>h</span>.<span style=color:#a6e22e>Sum64</span>()
</span></span><span style=display:flex><span><span style=color:#75715e>/*9*/</span> 		<span style=color:#a6e22e>buf</span> = []byte(<span style=color:#a6e22e>strconv</span>.<span style=color:#a6e22e>FormatUint</span>(<span style=color:#a6e22e>out</span>, <span style=color:#ae81ff>10</span>))
</span></span><span style=display:flex><span><span style=color:#75715e>/*10*/</span> 	}
</span></span><span style=display:flex><span><span style=color:#75715e>/*11*/</span> 	<span style=color:#66d9ef>return</span> <span style=color:#a6e22e>out</span>, <span style=color:#66d9ef>nil</span>
</span></span><span style=display:flex><span><span style=color:#75715e>/*12*/</span> }
</span></span></code></pre></div><p>I found:</p><ol><li>on line 9, I was allocating a string on every iteration of the loop</li><li>on line 4, I was allocating a byte array every time <code>cpuBoundWorkFunc</code> was called</li></ol><p>Whats more is after doing some escape analysis, I also found that the buffer being passed to <code>h.Write(buf)</code> was escaping to the heap:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-sh data-lang=sh><span style=display:flex><span>go build -gcflags<span style=color:#f92672>=</span>-m<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span> github.com/spaolacci/murmur3
</span></span><span style=display:flex><span>go test -gcflags<span style=color:#f92672>=</span>-m<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span> -c ./playground/parallelism_in_go/parallel_cpu_bound_2_test.go
</span></span></code></pre></div><p>This was apparently happening due to the internals of murmur3. Specifically somewhere internally a range was being taken from this byte slice. This is grounds for Go to allocate the byte array to the heap so it was escaping to the heap.</p><p>I wanted to eliminate heap allocations so I dreamed a new <code>Do</code> function that could maintain a shared state per worker goroutine:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#75715e>//parallel.go
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>DoWithState</span>[<span style=color:#a6e22e>I</span> <span style=color:#a6e22e>any</span>, <span style=color:#a6e22e>O</span> <span style=color:#a6e22e>any</span>, <span style=color:#a6e22e>S</span> <span style=color:#a6e22e>any</span>](<span style=color:#a6e22e>work</span> []<span style=color:#a6e22e>I</span>, <span style=color:#a6e22e>stateFunc</span> <span style=color:#a6e22e>StateFunc</span>[<span style=color:#a6e22e>S</span>], <span style=color:#a6e22e>workFunc</span> <span style=color:#a6e22e>WorkFuncWithState</span>[<span style=color:#a6e22e>S</span>, <span style=color:#a6e22e>I</span>, <span style=color:#a6e22e>O</span>], <span style=color:#a6e22e>workers</span> <span style=color:#66d9ef>int</span>, <span style=color:#a6e22e>bufferSize</span> <span style=color:#66d9ef>int</span>) []<span style=color:#a6e22e>Result</span>[<span style=color:#a6e22e>I</span>, <span style=color:#a6e22e>O</span>] {
</span></span><span style=display:flex><span>	<span style=color:#f92672>...</span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>var</span> <span style=color:#a6e22e>wg</span> <span style=color:#a6e22e>sync</span>.<span style=color:#a6e22e>WaitGroup</span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>wg</span>.<span style=color:#a6e22e>Add</span>(<span style=color:#a6e22e>workers</span>)
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>i</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>0</span>; <span style=color:#a6e22e>i</span> &lt; <span style=color:#a6e22e>workers</span>; <span style=color:#a6e22e>i</span><span style=color:#f92672>++</span> {
</span></span><span style=display:flex><span>		<span style=color:#66d9ef>go</span> <span style=color:#66d9ef>func</span>() {
</span></span><span style=display:flex><span>			<span style=color:#a6e22e>s</span> <span style=color:#f92672>:=</span> <span style=color:#a6e22e>stateFunc</span>() <span style=color:#75715e>// create a shared state when the worker goroutine starts.
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>			<span style=color:#66d9ef>defer</span> <span style=color:#a6e22e>wg</span>.<span style=color:#a6e22e>Done</span>()
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>w</span> <span style=color:#f92672>:=</span> <span style=color:#66d9ef>range</span> <span style=color:#a6e22e>workC</span> {
</span></span><span style=display:flex><span>				<span style=color:#a6e22e>o</span>, <span style=color:#a6e22e>err</span> <span style=color:#f92672>:=</span> <span style=color:#a6e22e>workFunc</span>(<span style=color:#a6e22e>s</span>, <span style=color:#a6e22e>w</span>) <span style=color:#75715e>// pass that shared state to the work function
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>				<span style=color:#a6e22e>resultC</span> <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>Result</span>[<span style=color:#a6e22e>I</span>, <span style=color:#a6e22e>O</span>]{<span style=color:#a6e22e>w</span>, <span style=color:#a6e22e>o</span>, <span style=color:#a6e22e>err</span>}
</span></span><span style=display:flex><span>			}
</span></span><span style=display:flex><span>		}()
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Then I created a new work function and a function to initialize the shared state:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>cpuBoundWorkFuncV3State</span>() []<span style=color:#66d9ef>byte</span> {
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> make([]<span style=color:#66d9ef>byte</span>, <span style=color:#ae81ff>8</span>)
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>cpuBoundWorkFuncV3</span>(<span style=color:#a6e22e>byteArray</span> []<span style=color:#66d9ef>byte</span>, <span style=color:#a6e22e>input</span> <span style=color:#66d9ef>uint64</span>) (<span style=color:#66d9ef>uint64</span>, <span style=color:#66d9ef>error</span>) {
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>const</span> <span style=color:#a6e22e>hashLoopCt</span> = <span style=color:#ae81ff>10000</span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>i</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>0</span>; <span style=color:#a6e22e>i</span> &lt; <span style=color:#a6e22e>hashLoopCt</span>; <span style=color:#a6e22e>i</span><span style=color:#f92672>++</span> {
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>binary</span>.<span style=color:#a6e22e>LittleEndian</span>.<span style=color:#a6e22e>PutUint64</span>(<span style=color:#a6e22e>byteArray</span>, <span style=color:#a6e22e>input</span>)
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>input</span> = <span style=color:#a6e22e>murmur3</span>.<span style=color:#a6e22e>Sum64</span>(<span style=color:#a6e22e>byteArray</span>)
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> <span style=color:#a6e22e>input</span>, <span style=color:#66d9ef>nil</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>My new work function would obviously not produce the same output, but it was more or less doing the same thing and this time avoiding allocations on every iteration of the loop. I hoped this would eliminate the noise factor which I hypothesized was coming from the GC.</p><p>Here were my results:</p><p><img src=../images/cpubound5.png alt></p><p><img src=../images/cpubound6.png alt></p><p>I noticed right away this strange peak at workers = 2 than hadn&rsquo;t cropped up before. I found this weird since it didn&rsquo;t make sense that an increase in parallelism was also increasing execution time! Oh well, it certainly revealed that this work function had very different characteristics to the last one!</p><p>Without fail however, I continued to see execution time trend downwards after workers > logical processors! It couldn&rsquo;t be alluded!</p><p>Once again I turned to cpu profiles, memory profiles and execution traces.</p><p>This time the memory trace was very boring. Hardly any memory was being allocated for the whole test. However when comparing CPU profiles between workers = 12 and workers = 24, I couldn&rsquo;t see any obvious reason why workers = 24 was faster than the other. I was so baffled by this result I started to wonder whether somehow ramping the workers up to 24 was actually &ldquo;warming&rdquo; up the CPU better, making the workers = 24 test run faster. To be sure, I actually ran the tests backwards:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>BenchmarkDoCPUBoundWorkV3Backwards</span>(<span style=color:#a6e22e>b</span> <span style=color:#f92672>*</span><span style=color:#a6e22e>testing</span>.<span style=color:#a6e22e>B</span>) {
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>workUnits</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>300</span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>maxWorkers</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>24</span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>bufferSize</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>3000</span>
</span></span><span style=display:flex><span>	<span style=color:#a6e22e>work</span> <span style=color:#f92672>:=</span> make([]<span style=color:#66d9ef>uint64</span>, <span style=color:#a6e22e>workUnits</span>)
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>i</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>0</span>; <span style=color:#a6e22e>i</span> &lt; <span style=color:#a6e22e>workUnits</span>; <span style=color:#a6e22e>i</span><span style=color:#f92672>++</span> {
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>work</span>[<span style=color:#a6e22e>i</span>] = uint64(<span style=color:#a6e22e>i</span>)
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>ws</span> <span style=color:#f92672>:=</span> <span style=color:#a6e22e>maxWorkers</span>; <span style=color:#a6e22e>ws</span> &gt; <span style=color:#ae81ff>0</span>; <span style=color:#a6e22e>ws</span><span style=color:#f92672>--</span> {
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>b</span>.<span style=color:#a6e22e>Run</span>(<span style=color:#a6e22e>fmt</span>.<span style=color:#a6e22e>Sprintf</span>(<span style=color:#e6db74>&#34;workers %d&#34;</span>, <span style=color:#a6e22e>ws</span>), <span style=color:#66d9ef>func</span>(<span style=color:#a6e22e>b</span> <span style=color:#f92672>*</span><span style=color:#a6e22e>testing</span>.<span style=color:#a6e22e>B</span>) {
</span></span><span style=display:flex><span>			<span style=color:#66d9ef>for</span> <span style=color:#a6e22e>i</span> <span style=color:#f92672>:=</span> <span style=color:#ae81ff>0</span>; <span style=color:#a6e22e>i</span> &lt; <span style=color:#a6e22e>b</span>.<span style=color:#a6e22e>N</span>; <span style=color:#a6e22e>i</span><span style=color:#f92672>++</span> {
</span></span><span style=display:flex><span>				<span style=color:#a6e22e>DoWithState</span>(<span style=color:#a6e22e>work</span>, <span style=color:#a6e22e>cpuBoundWorkFuncV3State</span>, <span style=color:#a6e22e>cpuBoundWorkFuncV3</span>, <span style=color:#a6e22e>ws</span>, <span style=color:#a6e22e>bufferSize</span>)
</span></span><span style=display:flex><span>			}
</span></span><span style=display:flex><span>		})
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Astonishingly I got the same results. For some unknown reason, workers = 24 just performed better.</p><p>I started to look into the CPU profiles of individual lines of the murmur hash function. This is where I got another hunch. It seemed that between 12 and 24 worker tests particular lines of code in murmur3 just performed better for workers = 24. I wondered if this might have something to do with alignment of memory and cache lines. Its impossible to know because I just don&rsquo;t have that much in-depth knowledge. However it gave me one lust hunch&mldr; perhaps murmur3 is just such a complex function itself, that its creating situations where for no apparent reason a higher number of workers favour it. This led me to one last test&mldr;</p><h3 id=experiment-4>Experiment 4</h3><p>No more murmur.. this time I made the simplest work function conceivable with 0 memory allocations:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-go data-lang=go><span style=display:flex><span><span style=color:#66d9ef>func</span> <span style=color:#a6e22e>cpuBoundWorkFuncV4</span>(<span style=color:#a6e22e>input</span> <span style=color:#66d9ef>uint64</span>) (<span style=color:#66d9ef>uint64</span>, <span style=color:#66d9ef>error</span>) {
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>var</span> <span style=color:#a6e22e>i</span> <span style=color:#66d9ef>uint64</span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>var</span> <span style=color:#a6e22e>x</span> <span style=color:#66d9ef>uint64</span>
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>for</span> ; <span style=color:#a6e22e>i</span> &lt; <span style=color:#ae81ff>1000000</span>; <span style=color:#a6e22e>i</span><span style=color:#f92672>++</span> {
</span></span><span style=display:flex><span>		<span style=color:#a6e22e>x</span> = <span style=color:#a6e22e>i</span> <span style=color:#f92672>%</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>	}
</span></span><span style=display:flex><span>	<span style=color:#66d9ef>return</span> <span style=color:#a6e22e>x</span>, <span style=color:#66d9ef>nil</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>This function does nothing more than operate in a tight loop. The only reason I added lines like <code>x = i % 2</code> was to prevent the go compiler from optimising out my variables. I thought go probably doesn&rsquo;t know how to optimize the result of <code>x</code> in this situation so this would be a good way to make the CPU do some work.</p><p>I ran the test and these were my results:</p><p><img src=../images/cpubound8.png alt>
<img src=../images/cpubound9.png alt>
<img src=../images/cpubound10.png alt></p><p>The test results showed something different this time. Instead of the execution time trending down as workers increased beyond 12, it was trending up. Still the optimium number of worker go routines was not 12, however at least I was starting to see that additional go routines was incurring a schedule penalty as predicted.</p><p>In fact the results were even more apparent when I ran it all the way up to 100 workers:
<img src=../images/cpubound11.png alt></p><p>This result got me pretty excited. So I ran with even more workers and performed 3 test runs to make sure I wasn&rsquo;t running into any noise from the machine:</p><p><img src=../images/cpubound12.png alt></p><p>.. and more workers:</p><p><img src=../images/cpubound13.png alt></p><p>Okay finally I could see what I was looking for! Taking the trendline each time, I found that there was a 0.0005ms (500μs) scheduler penalty per goroutine.
As for the magic number 12, I could not find it. In repeats of the test it just didn&rsquo;t show up:</p><p><img src=../images/cpubound14.png alt>
<img src=../images/cpubound15.png alt>
<img src=../images/cpubound16.png alt></p><p>However what I did notice is that there seems to be random noise in all the results. For example, comparing 3 individual back to back runs of the test, the results don&rsquo;t exactly align with each other:
<img src=../images/cpubound17.png alt></p><p>This suggested to me that perhaps its impossible to see the 500μs penalty between workers = 12 and workers = 13. Perhaps that just wasn&rsquo;t going to possible given that my personal computer always has some degree of noise.</p><h2 id=conclusion>Conclusion</h2><p>Although I coulodn&rsquo;t find the magic number workers = logical processors, I can see with a reasonable degree of confidence that workers = logical processors is the point at which you get the most reasonable optimization with CPU bound work.s</p><ul class=pa0></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href=https://chrisjpalmer.github.io>&copy; Tech Blog 2022</a><div><div class=ananke-socials></div></div></div></footer></body></html>